{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fb9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import os\n",
    "# load_dotenv()\n",
    "# from dotenv import load_dotenv, dotenv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483277a8",
   "metadata": {},
   "source": [
    "# File Reading / List population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc0435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set directory, get list of files in raw data directory, so that we can loop through them\n",
    "directory = '../data/raw_data'\n",
    "raw_data_files = os.listdir(directory)\n",
    "#Create list of dfs, that will hold pointers to each json -> df.\n",
    "activity_df_list = []\n",
    "sleep_df_list = []\n",
    "skin_df_list = []\n",
    "hrv_df_list = []\n",
    "brv_df_list = []\n",
    "#list of dfs from above, for referencing\n",
    "list_of_dfs = [activity_df_list, sleep_df_list, skin_df_list, hrv_df_list, brv_df_list]\n",
    "list_of_str = ['get_activities', 'get_sleep', 'get_skin', 'get_hrv', 'get_br']\n",
    "list_of_norm = ['activities', 'sleep', 'tempSkin', 'hrv', 'br']\n",
    "#Initialise the excel data dfs\n",
    "sleep_xsl_df = None\n",
    "stress_xsl_df = None\n",
    "mfp_df = None\n",
    "gf_df = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12187d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting json files in raw_data and then creating a df for each of them, adding them to a list\n",
    "\n",
    "for index, ldf in enumerate(list_of_dfs):\n",
    "    \n",
    "    #looping through files list, and creating a list of jsons by loading them all\n",
    "    list_of_dfs[index] = [json.load(open(directory + \"/\" + f)) for f in raw_data_files if f.startswith(list_of_str[index]) and f.endswith('.json')]\n",
    "    #print(len(ldf))\n",
    "    \n",
    "    #Loop and apply json_normalize on all files in ldf\n",
    "    list_of_dfs[index][:] = map(lambda x: pd.json_normalize(x[list_of_norm[index]]), list_of_dfs[index])\n",
    "    \n",
    "    #Convert normalized json dicts to dfs\n",
    "    list_of_dfs[index][:] = map(lambda x: pd.DataFrame.from_dict(x), list_of_dfs[index])\n",
    "    #print(list_of_dfs[index][0].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e1ac7",
   "metadata": {},
   "source": [
    "# Activity Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26b9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through activity files normalize, pd them, drop cols, drop more cols (might need a catch), concat all dfs ,\n",
    "# loop through and transform data, create column names, change types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864bca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets remove all the columns that we deemed not necessary.\n",
    "\n",
    "columns_remove = ['logId','activityLevel','logType','caloriesLink','heartRateLink','tcxLink','lastModified','hasGps','manualValuesSpecified.calories','manualValuesSpecified.distance','manualValuesSpecified.steps','activeZoneMinutes.totalMinutes','activeZoneMinutes.minutesInHeartRateZones','distance','speed','pace','distanceUnit', 'source.type', 'source.id', 'source.url', 'source.trackerFeatures', 'source.name', 'inProgress', 'customHeartRateZones']\n",
    "#Loop through list of dfs, and call the .drop func and remove listed columns above\n",
    "list_of_dfs[0][:] = [df.drop(columns=[col for col in columns_remove if col in df.columns], axis=1) for df in list_of_dfs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f074cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now concat all the data frames together\n",
    "\n",
    "list_of_dfs[0] = pd.concat(list_of_dfs[0], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b9c42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now expand some of the cols within the df\n",
    "\n",
    "#Give list_of_dfs[0] an alias for simplicity\n",
    "activity_df = list_of_dfs[0]\n",
    "\n",
    "#Initialize new column names in df\n",
    "base_column_string = 'hrz'\n",
    "activity_df['hrz_OutofRange_calories'] = None\n",
    "activity_df['hrz_FatBurn_calories'] = None\n",
    "activity_df['hrz_Cardio_calories'] = None\n",
    "activity_df['hrz_Peak_calories'] = None\n",
    "\n",
    "activity_df['hrz_OutofRange_minutes'] = None\n",
    "activity_df['hrz_FatBurn_minutes'] = None\n",
    "activity_df['hrz_Cardio_minutes'] = None\n",
    "activity_df['hrz_Peak_minutes'] = None\n",
    "\n",
    "#loop through df\n",
    "for index in range(0, len(activity_df['heartRateZones'])):\n",
    "    #get list of heartratezones from nested json structure\n",
    "    data_list = activity_df['heartRateZones'].iloc[index]\n",
    "    \n",
    "    #loop through the kv pair in each item in list mentioned above\n",
    "    for small_dict in data_list:\n",
    "        #generate what column the data will be placed in by the value \n",
    "        generated_base_col_string = base_column_string + '_' + small_dict['name']\n",
    "        generated_base_col_string = generated_base_col_string.replace(\" \", \"\")\n",
    "        \n",
    "        #get calorie value\n",
    "        cal_val = small_dict['caloriesOut']\n",
    "        #insert calories by col name generated and particular index\n",
    "        activity_df.iloc[index, activity_df.columns.get_loc(generated_base_col_string + '_' + 'calories')] = cal_val\n",
    "        \n",
    "        #Repeat process above but with minutes per section\n",
    "        minute_val = small_dict['minutes']\n",
    "        activity_df.iloc[index, activity_df.columns.get_loc(generated_base_col_string + '_' + 'minutes')] = minute_val\n",
    "        \n",
    "\n",
    "activity_df.drop(columns=['heartRateZones'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7743bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns that I outlined in document\n",
    "rename_act_cols = {\"customHeartRateZones\" : \"custom_hrz\", \n",
    "                   \"intervalWorkoutData.intervalSummaries\" : \"iwd_intervalSummaries\",\n",
    "                   \"intervalWorkoutData.numRepeats\" : \"iwd_numRepeats\"}\n",
    "\n",
    "list_of_dfs[0].rename(columns = rename_act_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430cac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data types \n",
    "data_type_change = {'hrz_OutofRange_calories' : 'float64',\n",
    "                    'hrz_FatBurn_calories' : 'float64',\n",
    "                    'hrz_Cardio_calories' : 'float64',\n",
    "                    'hrz_Peak_calories' : 'float64',\n",
    "                    'hrz_OutofRange_minutes' : 'int64',\n",
    "                    'hrz_FatBurn_minutes' : 'int64',\n",
    "                    'hrz_Cardio_minutes' : 'int64',\n",
    "                    'hrz_Peak_minutes' : 'int64'}\n",
    "\n",
    "list_of_dfs[0] = list_of_dfs[0].astype(data_type_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a003790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294 entries, 0 to 293\n",
      "Data columns (total 22 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   activityTypeId           294 non-null    int64  \n",
      " 1   activityName             294 non-null    object \n",
      " 2   calories                 294 non-null    int64  \n",
      " 3   steps                    293 non-null    float64\n",
      " 4   averageHeartRate         294 non-null    int64  \n",
      " 5   duration                 294 non-null    int64  \n",
      " 6   activeDuration           294 non-null    int64  \n",
      " 7   startTime                294 non-null    object \n",
      " 8   originalStartTime        294 non-null    object \n",
      " 9   originalDuration         294 non-null    int64  \n",
      " 10  elevationGain            294 non-null    float64\n",
      " 11  hasActiveZoneMinutes     294 non-null    bool   \n",
      " 12  iwd_intervalSummaries    294 non-null    object \n",
      " 13  iwd_numRepeats           294 non-null    int64  \n",
      " 14  hrz_OutofRange_calories  294 non-null    float64\n",
      " 15  hrz_FatBurn_calories     294 non-null    float64\n",
      " 16  hrz_Cardio_calories      294 non-null    float64\n",
      " 17  hrz_Peak_calories        294 non-null    float64\n",
      " 18  hrz_OutofRange_minutes   294 non-null    int64  \n",
      " 19  hrz_FatBurn_minutes      294 non-null    int64  \n",
      " 20  hrz_Cardio_minutes       294 non-null    int64  \n",
      " 21  hrz_Peak_minutes         294 non-null    int64  \n",
      "dtypes: bool(1), float64(6), int64(11), object(4)\n",
      "memory usage: 48.7+ KB\n"
     ]
    }
   ],
   "source": [
    "list_of_dfs[0].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999a655",
   "metadata": {},
   "source": [
    "# Sleep Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56da3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through sleep logs, keep only certain columns per each, concat all dfs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c9b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns \n",
    "remove_sleep_cols = ['infoCode', 'logId', 'logType', 'minutesAfterWakeup', 'minutesToFallAsleep']\n",
    "\n",
    "list_of_dfs[1][:] = [df.drop(columns = [col for col in remove_sleep_cols if col in df.columns], axis = 1) for df in list_of_dfs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aac22375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the dfs \n",
    "\n",
    "list_of_dfs[1] = pd.concat(list_of_dfs[1], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7e017c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 292 entries, 0 to 291\n",
      "Data columns (total 30 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   dateOfSleep                               292 non-null    object \n",
      " 1   duration                                  292 non-null    int64  \n",
      " 2   efficiency                                292 non-null    int64  \n",
      " 3   endTime                                   292 non-null    object \n",
      " 4   isMainSleep                               292 non-null    bool   \n",
      " 5   minutesAsleep                             292 non-null    int64  \n",
      " 6   minutesAwake                              292 non-null    int64  \n",
      " 7   startTime                                 292 non-null    object \n",
      " 8   timeInBed                                 292 non-null    int64  \n",
      " 9   type                                      292 non-null    object \n",
      " 10  levels.data                               292 non-null    object \n",
      " 11  levels.shortData                          281 non-null    object \n",
      " 12  levels.summary.deep.count                 281 non-null    float64\n",
      " 13  levels.summary.deep.minutes               281 non-null    float64\n",
      " 14  levels.summary.deep.thirtyDayAvgMinutes   281 non-null    float64\n",
      " 15  levels.summary.light.count                281 non-null    float64\n",
      " 16  levels.summary.light.minutes              281 non-null    float64\n",
      " 17  levels.summary.light.thirtyDayAvgMinutes  281 non-null    float64\n",
      " 18  levels.summary.rem.count                  281 non-null    float64\n",
      " 19  levels.summary.rem.minutes                281 non-null    float64\n",
      " 20  levels.summary.rem.thirtyDayAvgMinutes    281 non-null    float64\n",
      " 21  levels.summary.wake.count                 281 non-null    float64\n",
      " 22  levels.summary.wake.minutes               281 non-null    float64\n",
      " 23  levels.summary.wake.thirtyDayAvgMinutes   281 non-null    float64\n",
      " 24  levels.summary.asleep.count               11 non-null     float64\n",
      " 25  levels.summary.asleep.minutes             11 non-null     float64\n",
      " 26  levels.summary.awake.count                11 non-null     float64\n",
      " 27  levels.summary.awake.minutes              11 non-null     float64\n",
      " 28  levels.summary.restless.count             11 non-null     float64\n",
      " 29  levels.summary.restless.minutes           11 non-null     float64\n",
      "dtypes: bool(1), float64(18), int64(5), object(6)\n",
      "memory usage: 66.6+ KB\n"
     ]
    }
   ],
   "source": [
    "list_of_dfs[1].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa9f6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sleep csv, (go through date conversion stuff and only keep necessary things), \n",
    "# (dont drop first row, idk why that is there), create datetimekey col and convert to datetime..., perform join, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1e0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d7d470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform any necessary data cleaning as was done on sleep data csv..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f466d",
   "metadata": {},
   "source": [
    "# Skin / HRV / BRV Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd15c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy work dne in previous notebook, try to  find a way to auto detect the # of files with skin temp..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c44bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7408b059",
   "metadata": {},
   "source": [
    "# Stress Data Cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
