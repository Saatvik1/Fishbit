{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fb9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "# load_dotenv()\n",
    "# from dotenv import load_dotenv, dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc760c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For recreateability, not everyone is going to have this, so if recreate = True, then it will look for google form data\n",
    "\n",
    "recreate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483277a8",
   "metadata": {},
   "source": [
    "# File Reading / List population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc0435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set directory, get list of files in raw data directory, so that we can loop through them\n",
    "directory = '../data/raw_data'\n",
    "raw_data_files = os.listdir(directory)\n",
    "#Create list of dfs, that will hold pointers to each json -> df.\n",
    "activity_df_list = []\n",
    "sleep_df_list = []\n",
    "skin_df_list = []\n",
    "hrv_df_list = []\n",
    "brv_df_list = []\n",
    "#list of dfs from above, for referencing\n",
    "list_of_dfs = [activity_df_list, sleep_df_list, skin_df_list, hrv_df_list, brv_df_list]\n",
    "list_of_str = ['get_activities', 'get_sleep', 'get_skin', 'get_hrv', 'get_br']\n",
    "list_of_norm = ['activities', 'sleep', 'tempSkin', 'hrv', 'br']\n",
    "#Initialise the excel data dfs\n",
    "sleep_xsl_df = None\n",
    "stress_xsl_df = None\n",
    "mfp_df = None\n",
    "gf_df = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12187d2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Converting json files in raw_data and then creating a df for each of them, adding them to a list\n",
    "\n",
    "for index, ldf in enumerate(list_of_dfs):\n",
    "    \n",
    "    #looping through files list, and creating a list of jsons by loading them all\n",
    "    list_of_dfs[index] = [json.load(open(directory + \"/\" + f)) for f in raw_data_files if f.startswith(list_of_str[index]) and f.endswith('.json')]\n",
    "    \n",
    "    #Loop and apply json_normalize on all files in ldf\n",
    "    list_of_dfs[index][:] = map(lambda x: pd.json_normalize(x[list_of_norm[index]]), list_of_dfs[index])\n",
    "    \n",
    "    #Convert normalized json dicts to dfs\n",
    "    list_of_dfs[index][:] = map(lambda x: pd.DataFrame.from_dict(x), list_of_dfs[index])\n",
    "    #print(list_of_dfs[index][0].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e1ac7",
   "metadata": {},
   "source": [
    "# Activity Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26b9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through activity files normalize, pd them, drop cols, drop more cols (might need a catch), concat all dfs ,\n",
    "# loop through and transform data, create column names, change types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864bca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets remove all the columns that we deemed not necessary.\n",
    "\n",
    "columns_remove = ['logId','activityLevel','logType','caloriesLink','heartRateLink','tcxLink','lastModified','hasGps','manualValuesSpecified.calories','manualValuesSpecified.distance','manualValuesSpecified.steps','activeZoneMinutes.totalMinutes','activeZoneMinutes.minutesInHeartRateZones','distance','speed','pace','distanceUnit', 'source.type', 'source.id', 'source.url', 'source.trackerFeatures', 'source.name', 'inProgress', 'customHeartRateZones']\n",
    "#Loop through list of dfs, and call the .drop func and remove listed columns above\n",
    "list_of_dfs[0][:] = [df.drop(columns=[col for col in columns_remove if col in df.columns], axis=1) for df in list_of_dfs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f074cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now concat all the data frames together\n",
    "\n",
    "list_of_dfs[0] = pd.concat(list_of_dfs[0], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9c42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now expand some of the cols within the df\n",
    "\n",
    "#Give list_of_dfs[0] an alias for simplicity\n",
    "activity_df = list_of_dfs[0]\n",
    "\n",
    "#Initialize new column names in df\n",
    "base_column_string = 'hrz'\n",
    "activity_df['hrz_OutofRange_calories'] = None\n",
    "activity_df['hrz_FatBurn_calories'] = None\n",
    "activity_df['hrz_Cardio_calories'] = None\n",
    "activity_df['hrz_Peak_calories'] = None\n",
    "\n",
    "activity_df['hrz_OutofRange_minutes'] = None\n",
    "activity_df['hrz_FatBurn_minutes'] = None\n",
    "activity_df['hrz_Cardio_minutes'] = None\n",
    "activity_df['hrz_Peak_minutes'] = None\n",
    "\n",
    "#loop through df\n",
    "for index in range(0, len(activity_df['heartRateZones'])):\n",
    "    #get list of heartratezones from nested json structure\n",
    "    data_list = activity_df['heartRateZones'].iloc[index]\n",
    "    \n",
    "    #loop through the kv pair in each item in list mentioned above\n",
    "    for small_dict in data_list:\n",
    "        #generate what column the data will be placed in by the value \n",
    "        generated_base_col_string = base_column_string + '_' + small_dict['name']\n",
    "        generated_base_col_string = generated_base_col_string.replace(\" \", \"\")\n",
    "        \n",
    "        #get calorie value\n",
    "        cal_val = small_dict['caloriesOut']\n",
    "        #insert calories by col name generated and particular index\n",
    "        activity_df.iloc[index, activity_df.columns.get_loc(generated_base_col_string + '_' + 'calories')] = cal_val\n",
    "        \n",
    "        #Repeat process above but with minutes per section\n",
    "        minute_val = small_dict['minutes']\n",
    "        activity_df.iloc[index, activity_df.columns.get_loc(generated_base_col_string + '_' + 'minutes')] = minute_val\n",
    "        \n",
    "\n",
    "activity_df.drop(columns=['heartRateZones'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7743bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns that I outlined in document\n",
    "rename_act_cols = {\"customHeartRateZones\" : \"custom_hrz\", \n",
    "                   \"intervalWorkoutData.intervalSummaries\" : \"iwd_intervalSummaries\",\n",
    "                   \"intervalWorkoutData.numRepeats\" : \"iwd_numRepeats\"}\n",
    "\n",
    "list_of_dfs[0].rename(columns = rename_act_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430cac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data types \n",
    "data_type_change = {'hrz_OutofRange_calories' : 'float64',\n",
    "                    'hrz_FatBurn_calories' : 'float64',\n",
    "                    'hrz_Cardio_calories' : 'float64',\n",
    "                    'hrz_Peak_calories' : 'float64',\n",
    "                    'hrz_OutofRange_minutes' : 'int64',\n",
    "                    'hrz_FatBurn_minutes' : 'int64',\n",
    "                    'hrz_Cardio_minutes' : 'int64',\n",
    "                    'hrz_Peak_minutes' : 'int64'}\n",
    "\n",
    "list_of_dfs[0] = list_of_dfs[0].astype(data_type_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e02c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop some more columns\n",
    "list_of_dfs[0].drop(columns=['originalStartTime', 'originalDuration', 'elevationGain', 'hasActiveZoneMinutes', 'iwd_intervalSummaries', 'iwd_numRepeats'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a003790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 442 entries, 0 to 441\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   activityTypeId           442 non-null    int64  \n",
      " 1   activityName             442 non-null    object \n",
      " 2   calories                 442 non-null    int64  \n",
      " 3   steps                    436 non-null    float64\n",
      " 4   averageHeartRate         441 non-null    float64\n",
      " 5   duration                 442 non-null    int64  \n",
      " 6   activeDuration           442 non-null    int64  \n",
      " 7   startTime                442 non-null    object \n",
      " 8   hrz_OutofRange_calories  442 non-null    float64\n",
      " 9   hrz_FatBurn_calories     442 non-null    float64\n",
      " 10  hrz_Cardio_calories      442 non-null    float64\n",
      " 11  hrz_Peak_calories        442 non-null    float64\n",
      " 12  hrz_OutofRange_minutes   442 non-null    int64  \n",
      " 13  hrz_FatBurn_minutes      442 non-null    int64  \n",
      " 14  hrz_Cardio_minutes       442 non-null    int64  \n",
      " 15  hrz_Peak_minutes         442 non-null    int64  \n",
      "dtypes: float64(6), int64(8), object(2)\n",
      "memory usage: 55.4+ KB\n"
     ]
    }
   ],
   "source": [
    "list_of_dfs[0].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999a655",
   "metadata": {},
   "source": [
    "# Sleep Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56da3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through sleep logs, keep only certain columns per each, concat all dfs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c9b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns \n",
    "remove_sleep_cols = ['infoCode', 'logId', 'logType', 'minutesAfterWakeup', 'minutesToFallAsleep']\n",
    "\n",
    "list_of_dfs[1][:] = [df.drop(columns = [col for col in remove_sleep_cols if col in df.columns], axis = 1) for df in list_of_dfs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac22375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the dfs \n",
    "\n",
    "list_of_dfs[1] = pd.concat(list_of_dfs[1], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e017c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 352 entries, 0 to 351\n",
      "Data columns (total 30 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   dateOfSleep                               352 non-null    object \n",
      " 1   duration                                  352 non-null    int64  \n",
      " 2   efficiency                                352 non-null    int64  \n",
      " 3   endTime                                   352 non-null    object \n",
      " 4   isMainSleep                               352 non-null    bool   \n",
      " 5   minutesAsleep                             352 non-null    int64  \n",
      " 6   minutesAwake                              352 non-null    int64  \n",
      " 7   startTime                                 352 non-null    object \n",
      " 8   timeInBed                                 352 non-null    int64  \n",
      " 9   type                                      352 non-null    object \n",
      " 10  levels.data                               352 non-null    object \n",
      " 11  levels.shortData                          339 non-null    object \n",
      " 12  levels.summary.deep.count                 339 non-null    float64\n",
      " 13  levels.summary.deep.minutes               339 non-null    float64\n",
      " 14  levels.summary.deep.thirtyDayAvgMinutes   339 non-null    float64\n",
      " 15  levels.summary.light.count                339 non-null    float64\n",
      " 16  levels.summary.light.minutes              339 non-null    float64\n",
      " 17  levels.summary.light.thirtyDayAvgMinutes  339 non-null    float64\n",
      " 18  levels.summary.rem.count                  339 non-null    float64\n",
      " 19  levels.summary.rem.minutes                339 non-null    float64\n",
      " 20  levels.summary.rem.thirtyDayAvgMinutes    339 non-null    float64\n",
      " 21  levels.summary.wake.count                 339 non-null    float64\n",
      " 22  levels.summary.wake.minutes               339 non-null    float64\n",
      " 23  levels.summary.wake.thirtyDayAvgMinutes   339 non-null    float64\n",
      " 24  levels.summary.asleep.count               13 non-null     float64\n",
      " 25  levels.summary.asleep.minutes             13 non-null     float64\n",
      " 26  levels.summary.awake.count                13 non-null     float64\n",
      " 27  levels.summary.awake.minutes              13 non-null     float64\n",
      " 28  levels.summary.restless.count             13 non-null     float64\n",
      " 29  levels.summary.restless.minutes           13 non-null     float64\n",
      "dtypes: bool(1), float64(18), int64(5), object(6)\n",
      "memory usage: 80.2+ KB\n"
     ]
    }
   ],
   "source": [
    "list_of_dfs[1].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa9f6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sleep csv, (go through date conversion stuff and only keep necessary things), \n",
    "# (dont drop first row, idk why that is there), create datetimekey col and convert to datetime..., perform join, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86d1e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 276 entries, 0 to 275\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   sleep_log_entry_id     276 non-null    int64  \n",
      " 1   timestamp              276 non-null    object \n",
      " 2   overall_score          276 non-null    int64  \n",
      " 3   composition_score      276 non-null    int64  \n",
      " 4   revitalization_score   276 non-null    int64  \n",
      " 5   duration_score         276 non-null    int64  \n",
      " 6   deep_sleep_in_minutes  276 non-null    int64  \n",
      " 7   resting_heart_rate     276 non-null    int64  \n",
      " 8   restlessness           276 non-null    float64\n",
      "dtypes: float64(1), int64(7), object(1)\n",
      "memory usage: 19.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#Read csv\n",
    "sleep_score_df = pd.read_csv(\"../data/raw_data/takeout/Fitbit/Sleep Score/sleep_score.csv\")\n",
    "sleep_score_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f14e0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns that are not needed\n",
    "scd = ['sleep_log_entry_id', 'deep_sleep_in_minutes']\n",
    "\n",
    "sleep_score_df.drop(columns=scd, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb16430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create columns in both dfs, to match standard of datetime\n",
    "\n",
    "#API Format: 2023-11-24T10:51:30.000\n",
    "#CSV Format: 2023-11-24T10:51:30Z\n",
    "\n",
    "#Read as string, and get first 19 characters in both, set values for datetime_key column\n",
    "list_of_dfs[1]['datetime_key'] = list_of_dfs[1]['endTime'].str.slice(start=0, stop=19)\n",
    "sleep_score_df['datetime_key'] = sleep_score_df['timestamp'].str.slice(start=0, stop=19)\n",
    "\n",
    "#merge with left join, since API data should always be upto date, and csv download is scheduled for every two months\n",
    "list_of_dfs[1] = pd.merge(list_of_dfs[1], sleep_score_df, on=\"datetime_key\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "232c6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary datetime columns from old ones, since new key has good level of detail\n",
    "list_of_dfs[1].drop(columns=['timestamp', 'endTime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "497922e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to convert levels.data and levels.short. I need to make a seperate table for these columns, where each row would be \n",
    "# an element in their arrays. \n",
    "\n",
    "#Both arrays contain dateTime, level, seconds as key names in the dictionaries, and will be the column names, will have a \n",
    "#short column, true indicating its a short value and false indicating its a normal levels.data value\n",
    "#First lets make sure datetime_key has no duplicates, which it shouldnt\n",
    "\n",
    "list_of_dfs[1]['datetime_key'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69a4ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#They dont, so we can use that key as the keys for our array entries in our new table, so that we can associate them with\n",
    "#the correct date of sleep. \n",
    "\n",
    "#Create new df, initialize columns\n",
    "sleep_level_data = pd.DataFrame(columns = ['sleep_id', 'short', 'dateTime', 'level', 'seconds'])\n",
    "\n",
    "#Create sleep_id for list_of_dfs[1] for simplicity\n",
    "list_of_dfs[1]['sleep_id'] = range(1, len(list_of_dfs[1]) + 1)\n",
    "\n",
    "#Loop through sleep_ids, for each id, for each array per id, make each dictionary a row and append id to row\n",
    "for index, row in list_of_dfs[1].iterrows():\n",
    "\n",
    "    #get sleep_id of the row\n",
    "    sleep_id = row['sleep_id']\n",
    "    \n",
    "    #intialize array to store new dicts for every sleep_id\n",
    "    rows_to_add = []\n",
    "    \n",
    "    #loop through levels.data dicts in array per sleep session\n",
    "    for dic in row['levels.data']:\n",
    "        \n",
    "        #initialize temp dict to store data\n",
    "        dict_to_add = {'sleep_id' : sleep_id,\n",
    "                       'short' : False,\n",
    "                       'dateTime' : None,\n",
    "                       'level' : None,\n",
    "                       'seconds' : None}\n",
    "        \n",
    "        #loop through key, value pairs in each dict\n",
    "        for key, value in dic.items():\n",
    "            #Assign keys of array dict to temp dict to record values\n",
    "            dict_to_add[key] = value\n",
    "        \n",
    "        # Append new dict to array, which will be converted into a df\n",
    "        rows_to_add.append(dict_to_add)\n",
    "    \n",
    "    #Now loop through shortData\n",
    "    if type(row['levels.shortData']) == list:\n",
    "        for dic in row['levels.shortData']:\n",
    "\n",
    "            #initialize temp dict to store data\n",
    "            dict_to_add = {'sleep_id' : sleep_id,\n",
    "                           'short' : True,\n",
    "                           'dateTime' : None,\n",
    "                           'level' : None,\n",
    "                           'seconds' : None}\n",
    "\n",
    "            #loop through key, value pairs in each dict\n",
    "            for key, value in dic.items():\n",
    "                #Assign keys of array dict to temp dict to record values\n",
    "                dict_to_add[key] = value\n",
    "\n",
    "            # Append new dict to array, which will be converted into a df\n",
    "            rows_to_add.append(dict_to_add)\n",
    "        \n",
    "    #Convert rows_to_add to df, and concat to sleep_level_data\n",
    "    \n",
    "    temp = pd.DataFrame(rows_to_add)\n",
    "    sleep_level_data = pd.concat([sleep_level_data, temp], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5256528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15144 entries, 0 to 15143\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sleep_id  15144 non-null  int32 \n",
      " 1   short     15144 non-null  bool  \n",
      " 2   dateTime  15144 non-null  object\n",
      " 3   level     15144 non-null  object\n",
      " 4   seconds   15144 non-null  int64 \n",
      "dtypes: bool(1), int32(1), int64(1), object(2)\n",
      "memory usage: 429.0+ KB\n"
     ]
    }
   ],
   "source": [
    "sleep_level_data['sleep_id'] = sleep_level_data['sleep_id'].astype(int)\n",
    "sleep_level_data['short'] = sleep_level_data['short'].astype(bool)\n",
    "sleep_level_data['dateTime'] = sleep_level_data['dateTime'].astype(str)\n",
    "sleep_level_data['level'] = sleep_level_data['level'].astype(str)\n",
    "sleep_level_data['seconds'] = pd.to_numeric(sleep_level_data['seconds'], errors='coerce')  \n",
    "sleep_level_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47a82b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 352 entries, 0 to 351\n",
      "Data columns (total 34 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   dateOfSleep                   352 non-null    object \n",
      " 1   duration                      352 non-null    int64  \n",
      " 2   efficiency                    352 non-null    int64  \n",
      " 3   isMainSleep                   352 non-null    bool   \n",
      " 4   minutesAsleep                 352 non-null    int64  \n",
      " 5   minutesAwake                  352 non-null    int64  \n",
      " 6   startTime                     352 non-null    object \n",
      " 7   timeInBed                     352 non-null    int64  \n",
      " 8   sc_deep_count                 339 non-null    float64\n",
      " 9   sc_deep_minutes               339 non-null    float64\n",
      " 10  sc_deep_thirtyDayAvgMinutes   339 non-null    float64\n",
      " 11  sc_light_count                339 non-null    float64\n",
      " 12  sc_light_minutes              339 non-null    float64\n",
      " 13  sc_light_thirtyDayAvgMinutes  339 non-null    float64\n",
      " 14  sc_rem_count                  339 non-null    float64\n",
      " 15  sc_rem_minutes                339 non-null    float64\n",
      " 16  sc_rem_thirtyDayAvgMinutes    339 non-null    float64\n",
      " 17  sc_wake_count                 339 non-null    float64\n",
      " 18  sc_wake_minutes               339 non-null    float64\n",
      " 19  sc_wake_thirtyDayAvgMinutes   339 non-null    float64\n",
      " 20  sc_asleep_count               13 non-null     float64\n",
      " 21  sc_asleep_minutes             13 non-null     float64\n",
      " 22  sc_awake_count                13 non-null     float64\n",
      " 23  sc_awake_minutes              13 non-null     float64\n",
      " 24  sc_restless_count             13 non-null     float64\n",
      " 25  sc_restless_minutes           13 non-null     float64\n",
      " 26  datetime_key                  352 non-null    object \n",
      " 27  overall_score                 276 non-null    float64\n",
      " 28  composition_score             276 non-null    float64\n",
      " 29  revitalization_score          276 non-null    float64\n",
      " 30  duration_score                276 non-null    float64\n",
      " 31  resting_heart_rate            276 non-null    float64\n",
      " 32  restlessness                  276 non-null    float64\n",
      " 33  sleep_id                      352 non-null    int64  \n",
      "dtypes: bool(1), float64(24), int64(6), object(3)\n",
      "memory usage: 91.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#drop the unnecessary columns now\n",
    "list_of_dfs[1].drop(columns=['type', 'levels.data', 'levels.shortData'], inplace=True)\n",
    "#Convert all columns that have a . in the name to a _\n",
    "list_of_dfs[1].columns = list_of_dfs[1].columns.str.replace('.', '_')\n",
    "list_of_dfs[1].columns = list_of_dfs[1].columns.str.replace('levels_summary', 'sc')\n",
    "list_of_dfs[1].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f466d",
   "metadata": {},
   "source": [
    "# Skin / HRV / BRV Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd15c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For these three, all that needs to be done is concat all the dfs, then keep certain columns, thats all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33c44bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat the three in each of their sections\n",
    "\n",
    "for i in range (2,5):\n",
    "    list_of_dfs[i] = pd.concat(list_of_dfs[i], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e13ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only need to drop for brv\n",
    "\n",
    "list_of_dfs[2].drop(columns = ['logType'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57edbd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 334 entries, 0 to 333\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   dateTime               334 non-null    object \n",
      " 1   value.nightlyRelative  334 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 5.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 339 entries, 0 to 338\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   dateTime          339 non-null    object \n",
      " 1   value.dailyRmssd  339 non-null    float64\n",
      " 2   value.deepRmssd   339 non-null    float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 8.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 338 entries, 0 to 337\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   dateTime             338 non-null    object \n",
      " 1   value.breathingRate  338 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 5.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(list_of_dfs[2].info())\n",
    "print(list_of_dfs[3].info())\n",
    "print(list_of_dfs[4].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408b059",
   "metadata": {},
   "source": [
    "# Stress Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "965fc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns, convert all columns to lower, convert any types if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "211b7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_df = pd.read_csv('../data/raw_data/Takeout/Fitbit/Stress Score/Stress Score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ea01a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_drop_columns = ['DATE', 'MAX_SLEEP_POINTS', 'MAX_RESPONSIVENESS_POINTS', 'MAX_EXERTION_POINTS', 'STATUS', 'CALCULATION_FAILED']\n",
    "\n",
    "stress_df.drop(columns = stress_drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06f26bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_df.columns = stress_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8589741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 283 entries, 0 to 282\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   updated_at             281 non-null    object\n",
      " 1   stress_score           283 non-null    int64 \n",
      " 2   sleep_points           283 non-null    int64 \n",
      " 3   responsiveness_points  283 non-null    int64 \n",
      " 4   exertion_points        283 non-null    int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 11.2+ KB\n"
     ]
    }
   ],
   "source": [
    "stress_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ee4f1",
   "metadata": {},
   "source": [
    "# MyFitnessPal Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63408621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column names as specified in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ed4c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get filename from list of files in directory\n",
    "filename = None\n",
    "for f in raw_data_files:\n",
    "    if f.startswith(\"Nutrition\"):\n",
    "        filename = f\n",
    "\n",
    "mfp_df = pd.read_csv('../data/raw_data/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d69ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "mfp_rename = {'Date' : 'date', 'Meal' : 'meal', 'Calories' : 'calories', 'Fat (g)' : 'fat_g', \n",
    "              'Saturated Fat' : 'sat_fat', 'Polyunsaturated Fat' : 'poly_fat', 'Monounsaturated Fat' : 'mono_fat',\n",
    "              'Trans Fat' : 'trans_fat', 'Cholesterol' : 'cholesterol', 'Sodium (mg)' : 'sodium_mg', \n",
    "              'Potassium' : 'potassium', 'Carbohydrates (g)' : 'carbohydrates_g', 'Fiber' : 'fiber_g', \n",
    "              'Sugar' : 'sugar', 'Protein (g)' : 'protein_g', 'Vitamin A' : 'vitamin_a', 'Vitamin C' : 'vitamin_c', \n",
    "              'Calcium' : 'calcium', 'Iron' : 'iron'}\n",
    "\n",
    "mfp_df.rename(columns = mfp_rename, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8a579fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp_df.drop(columns = ['Note'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0648283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85 entries, 0 to 84\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   date             85 non-null     object \n",
      " 1   meal             85 non-null     object \n",
      " 2   calories         85 non-null     float64\n",
      " 3   fat_g            85 non-null     float64\n",
      " 4   sat_fat          85 non-null     float64\n",
      " 5   poly_fat         85 non-null     float64\n",
      " 6   mono_fat         85 non-null     float64\n",
      " 7   trans_fat        85 non-null     float64\n",
      " 8   cholesterol      85 non-null     float64\n",
      " 9   sodium_mg        85 non-null     float64\n",
      " 10  potassium        85 non-null     float64\n",
      " 11  carbohydrates_g  85 non-null     float64\n",
      " 12  fiber_g          85 non-null     float64\n",
      " 13  sugar            85 non-null     float64\n",
      " 14  protein_g        85 non-null     float64\n",
      " 15  vitamin_a        85 non-null     float64\n",
      " 16  vitamin_c        85 non-null     float64\n",
      " 17  calcium          85 non-null     float64\n",
      " 18  iron             85 non-null     float64\n",
      "dtypes: float64(17), object(2)\n",
      "memory usage: 12.7+ KB\n"
     ]
    }
   ],
   "source": [
    "mfp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347698a",
   "metadata": {},
   "source": [
    "# Google Form Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb438857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns, expand colum for one of the data cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cb2c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not recreate:\n",
    "    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59f4a2fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gf_df = pd.read_csv('../data/raw_data/Fishbit_GF_Responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "300c3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns\n",
    "gf_df.columns = ['timestamp','date', 'last_sleep_feel', 'last_sleep_score', 'daily_particular_qualities', 'day_feel', \n",
    "                 'ef_status', 'ef_description', 'dissatisfied_status', 'stress_mgmt_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d445c465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'date', 'last_sleep_feel', 'last_sleep_score',\n",
      "       'daily_particular_qualities', 'day_feel', 'ef_status', 'ef_description',\n",
      "       'dissatisfied_status', 'stress_mgmt_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(gf_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7160767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the 'stress_mgmt_score' column, after dropping null\n",
    "split_cols = gf_df['stress_mgmt_score'].dropna().str.split(',', expand=True)\n",
    "\n",
    "# assign first value back to stress_mgmt_score\n",
    "gf_df.loc[split_cols.index, 'stress_mgmt_score'] = split_cols[0]\n",
    "\n",
    "# assign the rest of the columns to their values \n",
    "gf_df.loc[split_cols.index, 'stress_responsiveness'] = split_cols[1]\n",
    "gf_df.loc[split_cols.index, 'stress_exertion'] = split_cols[2]\n",
    "gf_df.loc[split_cols.index, 'stress_sleep'] = split_cols[3]\n",
    "\n",
    "# fill the inbetween values with nan\n",
    "gf_df[['stress_responsiveness', 'stress_exertion', 'stress_sleep']] = gf_df[['stress_responsiveness', 'stress_exertion', 'stress_sleep']].fillna(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f948f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype_change = {'stress_mgmt_score':'int64',\n",
    "                   'stress_responsiveness':'int64',\n",
    "                   'stress_exertion':'int64',\n",
    "                   'stress_sleep':'int64',\n",
    "                  }\n",
    "\n",
    "# gf_df = gf_df.astype(datatype_change)\n",
    "\n",
    "#Force the types to be Int64, not int64... today I learned that pandas Int64 is different and supports null unlike the int64 based off numpy\n",
    "\n",
    "gf_df['stress_mgmt_score'] = pd.to_numeric(gf_df['stress_mgmt_score'], errors='coerce').astype('Int64')\n",
    "gf_df['stress_responsiveness'] = pd.to_numeric(gf_df['stress_responsiveness'], errors='coerce').astype('Int64')\n",
    "gf_df['stress_exertion'] = pd.to_numeric(gf_df['stress_exertion'], errors='coerce').astype('Int64')\n",
    "gf_df['stress_sleep'] = pd.to_numeric(gf_df['stress_sleep'], errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4b06289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   timestamp                   51 non-null     object \n",
      " 1   date                        51 non-null     object \n",
      " 2   last_sleep_feel             44 non-null     object \n",
      " 3   last_sleep_score            37 non-null     float64\n",
      " 4   daily_particular_qualities  44 non-null     object \n",
      " 5   day_feel                    44 non-null     object \n",
      " 6   ef_status                   44 non-null     object \n",
      " 7   ef_description              14 non-null     object \n",
      " 8   dissatisfied_status         43 non-null     object \n",
      " 9   stress_mgmt_score           1 non-null      Int64  \n",
      " 10  stress_responsiveness       1 non-null      Int64  \n",
      " 11  stress_exertion             1 non-null      Int64  \n",
      " 12  stress_sleep                1 non-null      Int64  \n",
      "dtypes: Int64(4), float64(1), object(8)\n",
      "memory usage: 5.5+ KB\n"
     ]
    }
   ],
   "source": [
    "gf_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f06b41",
   "metadata": {},
   "source": [
    "# Export all DFs to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0e7a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we just export all of the final dfs as a csv to the interim data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32bbf72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of the dfs need proper date sorting:\n",
    "\n",
    "#br dateTime column needs to be sorted\n",
    "#hrv dateTime col too\n",
    "#tempSkin dateTime too\n",
    "\n",
    "#list_of_norm = ['activities', 'sleep', 'tempSkin', 'hrv', 'br']\n",
    "list_of_dfs[2].sort_values(by=\"dateTime\", inplace=True)\n",
    "list_of_dfs[3].sort_values(by=\"dateTime\", inplace=True)\n",
    "list_of_dfs[4].sort_values(by=\"dateTime\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "197b6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through list_of_dfs and convert each index into a csv\n",
    "for index, df in enumerate(list_of_dfs):\n",
    "    df.to_csv(\"../data/interim/initial_clean_\" + list_of_norm[index] + \".csv\", index=False)\n",
    "\n",
    "#convert mfp and gf to csv\n",
    "mfp_df.to_csv(\"../data/interim/initial_clean_mfp.csv\", index = False)\n",
    "gf_df.to_csv(\"../data/interim/initial_clean_gf.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65c90915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_level_data.to_csv(\"../data/interim/intial_clean_sleep_levels.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4f0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
